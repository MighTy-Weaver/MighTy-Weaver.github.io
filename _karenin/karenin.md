### Hi there
I’m Jiefu Ou, currently a final year undergraduate at HKUST, with double major in Computer Science (COSC) and Mathematics and Economics (MAEC). I'm keen on **Natural Language Processing (NLP)**, and I'm broadly applying to M.S./Ph.D. programs with a focus on AI/NLP.  
I'm proudly and fortunately being advised by Prof. [Yangqiu Song](https://www.cse.ust.hk/~yqsong/) at [Knowledge Computation Group@HKUST](https://hkust-knowcomp.github.io/ASER//about/), and Prof. [Benjamin Van Durme](https://www.cs.jhu.edu/~vandurme/) at [Center for Language and Speech Processing](https://www.clsp.jhu.edu/), JHU.

#### Research Foci and Interests
Specifically, my previous and ongoing research mainly investigate the following topics:<br>
* **Knowledge Graph Constuction and Completion**: Rule mining for missing relation prediction and instantiation; Discourse-based event-relation extractor for KG reconstruction.  Applied on [ASER](https://hkust-knowcomp.github.io/ASER/): a large-scale eventuality knowledge 
* **Discourse Analysis**: A neural [model](https://www.ijcai.org/Proceedings/2020/530) for implicit discourse relation classification; And the investigation of the role of discourse in argumentation.
* **Ontology-constrained/guided natural langauge processing**: Investigating the use of [FrameNet](https://framenet.icsi.berkeley.edu/fndrupal/about) in guiding the nerual model for text geration. Proposed two guiding/constraining methods at training and decoding phase respectively.  

Envisioning the future, I am enthusiastic about seeking answers/solutions to the following questions:  
* **Commonsense Knowledge for NLP**：The use of natural language by human beings are supported by vast amount of world knowledge that are not explicitly expressed in the context but assumed to be known by default. However, many current NLP models rely mostly on the explicit context in tackling NLP tasks. Whether and how would NLP models be benefit from the access of commensense knowledge, and what are the frameworks/methodologies that could efficiently approach this goal?

* **Structrual Linguistic Formalism for NLP**: Linguistic theories and formalisms like [discourse semantics](https://www.seas.upenn.edu/~pdtb/) and frame semantics are shown to be useful in several NLP tasks that I worked/am working on in my researc projects. Could we take this more broadly? These theories/formalisms reveal complicated underlying information of structrue and meaning of natural language that help people understand and use language properly. Whether and how could the injection of these information makes NLP models more powerful and interpretable?

#### Publications:
Xin Liu, **Jiefu Ou**, Yangqiu Song, and Xin Jiang. On the Importance of Word and Sentence Representation Learning in Implicit Discourse Relation Classification. In International Joint Conference on Artificial Intelligence (IJCAI), 2020. [pdf](https://arxiv.org/abs/2004.12617)

**Jiefu Ou\***, Anton Belyy*, Nathaniel Weir*, Felix Yu, and Benjamin Van Durme. InFillmore: Neural Frame Lexicalization for Narrative Text Infilling. Preprint. [url](http://jefferyo.github.io/pub/infillmore.html) 
(* indicates equal contribution)

Xin Liu, **Jiefu Ou**, Yangqiu Song, and Xin Jiang. Finetuning Pretrained Encoders with Discourse Context for Classification. To be submitted to ACL 2021. [url](http://jefferyo.github.io/pub/encodedisc.html)

Hongming Zhang*, Xin Liu*, Haojie Pan*, Haowen Ke, **Jiefu Ou**, Tianqing Fang, and Yangqiu Song. ASER: Commonsense Knowledge Acquisition with Higher-order Selectional Preference over Eventualities. To be submitted to JAIR. [url](http://jefferyo.github.io/pub/aser.html)
(* indicates equal contribution)